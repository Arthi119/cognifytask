# -*- coding: utf-8 -*-
"""cognifyz.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10tF6Vkpmn1_P_R_I5Zd1NhSPrSa6cqRM

TASK 1
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score

#Load the dataset
df = pd.read_csv("restaurant_data.csv")

#Get the column names
columns_list = df.columns.tolist()

#print the columns names
print("Columns in the dataset:")
print(columns_list)

#Select relevant features and target variable
X = df.drop("Aggregate rating", axis=1)
y = df["Aggregate rating"]

#print non numeric values
non_numeric_columns = df.select_dtypes(exclude=['float64', 'int64']).columns
print("Non-numeric columns:", non_numeric_columns)

# Drop the 'Cuisines' column (if it is non-numeric and not used for prediction)
df = df.drop('Cuisines', axis=1)

#Encode categorical variables using one-hot encoding
X_encoded = pd.get_dummies(X, drop_first=True)

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

#Train the Decision Tree Regression model
model = DecisionTreeRegressor()
model.fit(X_train, y_train)

#Make predictions on the testing data
y_pred = model.predict(X_test)

#Evaluate the model's performance using Mean Squared Error (MSE) and R-squared (R2)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error (MSE):", mse)
print("R-squared (R2):", r2)

#Analyze the most influential features affecting restaurant ratings (optional)
#Decision trees allow you to interpret feature importance easily. You can analyze the feature_importances_ attribute of the model.
importance = model.feature_importances_
feature_names = X_encoded.columns
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})
sorted_features = feature_importance_df.sort_values(by='Importance', ascending=False)

print("\nMost Influential Features:")
print(sorted_features)

"""TASK 2"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

#Load the dataset and handle missing values (if any)
df = pd.read_csv("restaurant_data.csv")

# Convert 'Cuisines' column to lowercase for case-insensitive matching
df['Cuisines'] = df['Cuisines'].str.lower()

# Encode categorical variables
le = LabelEncoder()
df['Cuisine_Encoded'] = le.fit_transform(df['Cuisines'])

# Define features and target variable
X = df[['Cuisine_Encoded']]
y = df['Aggregate rating']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Implement Decision Tree Regressor
model = DecisionTreeRegressor()
model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model.predict(X_test)

# Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error (MSE):", mse)
print("R-squared (R2):", r2)

# Make a prediction for sample user preferences
sample_user_preferences = 'Italian, Chinese'  # Sample user preferences for cuisine
sample_user_preferences = sample_user_preferences.lower()
sample_user_pref_encoded = le.transform([sample_user_preferences])[0]

predicted_rating = model.predict([[sample_user_pref_encoded]])[0]

print(f"Predicted Rating for User Preferences: {sample_user_preferences}: {predicted_rating:.2f}")

"""TASK3"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Generate a synthetic dataset for demonstration
np.random.seed(42)
num_samples = 1000
num_features = 5
X = np.random.randn(num_samples, num_features)
y = np.random.randint(2, size=num_samples)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the logistic regression model
logistic_model = LogisticRegression()

# Train the model using the training data
logistic_model.fit(X_train, y_train)

# Predict on the testing data
y_pred = logistic_model.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
confusion = confusion_matrix(y_test, y_pred)

# Print the evaluation metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("Confusion Matrix:\n", confusion)

"""TASK 4"""

import pandas as pd
import folium
from folium import plugins

#Step:1 Load the dataset
df=pd.read_csv("restaurant_data.csv")

# Step 2: Explore the latitude and longitude coordinates of the restaurants and visualize their distribution on a map
# Create a map centered at the mean latitude and longitude
map_center = [df['Latitude'].mean(), df['Longitude'].mean()]
map_restaurants = folium.Map(location=map_center, zoom_start=12)

# Add markers for each restaurant on the map
for index, row in df.iterrows():
    popup_text = f"Restaurant: {row['Restaurant Name']}<br>Location: {row['Locality']}, {row['City']}<br>Cuisines: {row['Cuisines']}<br>Rating: {row['Aggregate rating']}"
    folium.Marker([row['Latitude'], row['Longitude']], popup=popup_text).add_to(map_restaurants)

# Display the map
map_restaurants.save("restaurant_map.html")

# Step 3: Group the restaurants by city or locality and analyze the concentration of restaurants in different areas
# Group by city and count the number of restaurants in each city
restaurant_count_by_city = df.groupby('City').size().reset_index(name='Restaurant Count')

# Display the restaurant count by city
print(restaurant_count_by_city)

# Step 4: Calculate statistics such as the average ratings, cuisines, or price ranges by city or locality
# Group by city and calculate the average rating for each city
average_rating_by_city = df.groupby('City')['Aggregate rating'].mean().reset_index(name='Average Rating')

# Display the average rating by city
print(average_rating_by_city)

# For example, you can use visualization libraries like seaborn to plot the distribution of ratings by city.
import seaborn as sns
import matplotlib.pyplot as plt

sns.boxplot(x='City', y='Aggregate rating', data=df)
plt.xticks(rotation=90)
plt.title('Distribution of Ratings by City')
plt.show()